KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt
KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt
KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt
KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt
Exception in thread Thread-11:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/rom/Desktop/Projects/mobile-robot-hl/src/mobile_robot_hl/mobile_robot_hl/trainer/trainer.py", line 215, in training_loop
    self.algorithm.train_one_epoch(self)
  File "/home/rom/Desktop/Projects/mobile-robot-hl/src/mobile_robot_hl/mobile_robot_hl/trainer/algorithms.py", line 809, in train_one_epoch
    self.logger.log(DataType.num, critic_2_loss.item(), key = "loss/critic2")
  File "/home/rom/Desktop/Projects/mobile-robot-hl/src/mobile_robot_hl/mobile_robot_hl/logger/logger.py", line 66, in log
    wandb.log(dictionary)
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1345, in log
    self.history._row_add(data)
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/wandb_history.py", line 44, in _row_add
    self._flush()
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/wandb_history.py", line 59, in _flush
    self._callback(row=self._data, step=self._step)
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 1023, in _history_callback
    self._backend.interface.publish_history(
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 501, in publish_history
    self._publish_history(history)
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface_queue.py", line 74, in _publish_history
    self._publish(rec)
  File "/home/rom/.local/lib/python3.8/site-packages/wandb/sdk/interface/interface_queue.py", line 223, in _publish
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt
{'start_training__RL': {'training_type': 'RL', 'algorithm_name': 'TD3_INTER', 'save_every': 1, 'max_epochs': 1, 'additional_algorithm_kwargs': {'run_name': 'test', 'run_id': 0, 'checkpoint_every': 10, 'device1': 'cuda:0', 'device2': 'cuda:1', 'discount': 0.95, 'tau': 0.2, 'noise': 0.3, 'exp_decay_const': 0.2, 'logger_name': 'WandbLogger'}}}
>>> =================Epoch 1=================
Run No. 1
Episode Length = 94
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1266,  1.1195,  1.1214,  1.1285,  1.1302,  1.1298,  1.1277,  1.1333,
         1.1273,  1.1292,  1.1160,  1.1326,  1.1302,  1.1295,  1.1235,  1.1234,
         1.1207,  1.1241,  1.1312,  1.1340,  1.1271,  1.1237,  1.1266,  1.1165,
         1.1319,  1.1352,  1.1366,  1.1338,  1.1372,  1.1329,  1.1316,  1.1359,
         1.1324,  1.1302,  1.1333,  1.1324,  1.1333,  1.1312,  1.1303,  1.1187,
         1.1176,  1.1228,  1.1228, -5.0000,  0.0000,  0.0000,  0.0000,  0.1279,
         1.1259,  1.1265,  1.1333,  1.0000], device='cuda:0')
target_q_episode tensor([17.2454, 17.1004, 16.9478, 16.7872, 16.6181, 16.4401, 16.2527, 16.0555,
        15.8479, 15.6294, 15.3994, 15.1572, 14.9023, 14.6340, 14.3516, 14.0543,
        13.7414, 13.4120, 13.0653, 12.7003, 12.3161, 11.9117, 11.4860, 11.0379,
        10.5662, 10.0696,  9.5470,  8.9968,  8.4177,  7.8081,  7.1664,  6.4910,
         5.7800,  5.0316,  4.2438,  3.4145,  2.5416,  1.6227,  0.6555, -0.3627,
        -1.4344, -2.5625, -3.7500, -5.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         3.7099,  2.8525,  1.9500,  1.0000], device='cuda:0')
target_q tensor([17.2454, 17.1004, 16.9478, 16.7872, 16.6181, 16.4401, 16.2527, 16.0555,
        15.8479, 15.6294, 15.3994, 15.1572, 14.9023, 14.6340, 14.3516, 14.0543,
        13.7414, 13.4120, 13.0653, 12.7003, 12.3161, 11.9117, 11.4860, 11.0379,
        10.5662, 10.0696,  9.5470,  8.9968,  8.4177,  7.8081,  7.1664,  6.4910,
         5.7800,  5.0316,  4.2438,  3.4145,  2.5416,  1.6227,  0.6555, -0.3627,
        -1.4344, -2.5625, -3.7500, -5.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         3.7099,  2.8525,  1.9500,  1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 0.0759, -0.2832,  0.0074],
        [ 0.0814, -0.2848,  0.0077],
        [ 0.0837, -0.2803,  0.0092],
        [ 0.0864, -0.2916,  0.0062],
        [ 0.0865, -0.2990,  0.0086],
        [ 0.0882, -0.3330,  0.0043],
        [ 0.0852, -0.3078,  0.0103],
        [ 0.0835, -0.2860,  0.0118],
        [ 0.0734, -0.2498,  0.0617],
        [ 0.0613, -0.1941,  0.1645],
        [ 0.0649, -0.2371,  0.0811],
        [ 0.0682, -0.2201,  0.0633],
        [ 0.0750, -0.2558,  0.0239],
        [ 0.0793, -0.2691,  0.0157],
        [ 0.0806, -0.2678,  0.0143],
        [ 0.0767, -0.2410,  0.0162],
        [ 0.0878, -0.2804,  0.0064],
        [ 0.0848, -0.3024,  0.0108],
        [ 0.0873, -0.3169,  0.0087],
        [ 0.0897, -0.3161,  0.0052],
        [ 0.0894, -0.3083,  0.0067],
        [ 0.0882, -0.3321,  0.0077],
        [ 0.0839, -0.3236,  0.0121],
        [ 0.0856, -0.2844,  0.0270],
        [ 0.0854, -0.3209,  0.0126],
        [ 0.0869, -0.3214,  0.0184],
        [ 0.0876, -0.3453,  0.0072],
        [ 0.0887, -0.3280,  0.0070],
        [ 0.0838, -0.3243,  0.0262],
        [ 0.0900, -0.3680,  0.0106],
        [ 0.0914, -0.3508,  0.0090],
        [ 0.0810, -0.3253,  0.0273],
        [ 0.0892, -0.3562,  0.0186],
        [ 0.0943, -0.3877,  0.0034],
        [ 0.0948, -0.3881,  0.0028],
        [ 0.0962, -0.3911,  0.0033],
        [ 0.0963, -0.3916,  0.0018],
        [ 0.0963, -0.3843,  0.0035],
        [ 0.0920, -0.3756,  0.0085],
        [ 0.0889, -0.3448,  0.0310],
        [ 0.0841, -0.3335,  0.0640],
        [ 0.0692, -0.2575,  0.2556],
        [ 0.0742, -0.2551,  0.0173],
        [ 0.0727, -0.2477,  0.0222],
        [ 0.0785, -0.3314,  0.0118],
        [ 0.0774, -0.2465,  0.0189],
        [ 0.0773, -0.2982,  0.0100],
        [ 0.0836, -0.2988,  0.0062],
        [ 0.0814, -0.2939,  0.0087],
        [ 0.0844, -0.2984,  0.0099],
        [ 0.0854, -0.3257,  0.0063],
        [ 0.0781, -0.2765,  0.0214],
        [ 0.0793, -0.2385,  0.0192],
        [ 0.0816, -0.3093,  0.0102],
        [ 0.0847, -0.3225,  0.0077],
        [ 0.0804, -0.3269,  0.0127],
        [ 0.0805, -0.3272,  0.0129],
        [ 0.0719, -0.2600,  0.0370],
        [ 0.0831, -0.2767,  0.0292],
        [ 0.0791, -0.1969,  0.0226],
        [ 0.0900, -0.2223,  0.0095],
        [ 0.0875, -0.1957,  0.0126],
        [ 0.0865, -0.2383,  0.0155],
        [ 0.0902, -0.3449,  0.0032],
        [ 0.0847, -0.3066,  0.0129],
        [ 0.0808, -0.2707,  0.0261],
        [ 0.0804, -0.1620,  0.0369],
        [ 0.0863, -0.2090,  0.0330],
        [ 0.0806, -0.2609,  0.0357],
        [ 0.0799, -0.2658,  0.0266],
        [ 0.0785, -0.2825,  0.0429],
        [ 0.0846, -0.3165,  0.0188],
        [ 0.0866, -0.3144,  0.0172],
        [ 0.0772, -0.2852,  0.0348],
        [ 0.0788, -0.3245,  0.0264],
        [ 0.0877, -0.3492,  0.0105],
        [ 0.0831, -0.2874,  0.0693],
        [ 0.0874, -0.3639,  0.0145],
        [ 0.0864, -0.3213,  0.0233],
        [ 0.0945, -0.3933,  0.0040],
        [ 0.0946, -0.3845,  0.0063],
        [ 0.0922, -0.3758,  0.0074],
        [ 0.0862, -0.3490,  0.0251],
        [ 0.0933, -0.3812,  0.0099],
        [ 0.0892, -0.3790,  0.0118],
        [ 0.0965, -0.3976,  0.0016],
        [ 0.0957, -0.3982,  0.0021],
        [ 0.0968, -0.3878,  0.0046],
        [ 0.0956, -0.3876,  0.0147],
        [ 0.0958, -0.3882,  0.0140],
        [ 0.0890, -0.3578,  0.0373],
        [ 0.0825, -0.3371,  0.0608],
        [ 0.0815, -0.3590,  0.0350],
        [ 0.0786, -0.3349,  0.1057]], device='cuda:0', grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 11. Optimize actor
# 12. Update target networks
Run No. 2
Episode Length = 81
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1319,  1.1364,  1.1265,  1.1251,  1.1306,  1.1360,  1.1305,  1.1304,
         1.1260,  1.1260,  1.1258, -5.0000,  0.0000,  0.0000,  0.0000,  0.1333,
         1.1273,  1.1333,  1.1311,  1.1287,  1.1280,  1.1289,  1.1345,  1.1401,
         1.1402,  1.1381,  1.1352,  1.1272, -7.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.1415,  1.1388,  1.1430,  1.1351,  1.1357,  1.1321,
         1.1354,  1.1376,  1.0000], device='cuda:0')
target_q_episode tensor([ 5.7800,  5.0316,  4.2438,  3.4145,  2.5416,  1.6227,  0.6555, -0.3627,
        -1.4344, -2.5625, -3.7500, -5.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         5.4103,  4.6424,  3.8341,  2.9833,  2.0876,  1.1449,  0.1525, -0.8921,
        -1.9917, -3.1491, -4.3675, -5.6500, -7.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  6.7316,  6.0333,  5.2982,  4.5244,  3.7099,
         2.8525,  1.9500,  1.0000], device='cuda:0')
target_q tensor([ 4.9374,  4.3255,  3.6787,  2.9995,  2.2858,  1.5345,  0.7416, -0.0920,
        -0.9703, -1.8939, -2.8662, -5.0000,  0.0000,  0.0000,  0.0000,  0.0242,
         4.6339,  4.0063,  3.3441,  2.6471,  1.9137,  1.1420,  0.3305, -0.5237,
        -1.4240, -2.3720, -3.3700, -4.4215, -7.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0256,  5.7178,  5.1468,  4.5435,  3.9101,  3.2426,
         2.5412,  1.8027,  1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 9.3854e-02, -3.6287e-01,  1.8350e-03],
        [ 9.3151e-02, -3.6418e-01,  1.9352e-03],
        [ 9.5341e-02, -3.7585e-01,  1.5767e-03],
        [ 9.5749e-02, -3.8767e-01,  8.5300e-04],
        [ 9.5921e-02, -3.8928e-01,  8.3572e-04],
        [ 9.7219e-02, -3.9237e-01,  3.6904e-04],
        [ 9.6435e-02, -3.8447e-01,  5.6362e-04],
        [ 9.6810e-02, -3.8852e-01,  5.7423e-04],
        [ 9.6911e-02, -3.8734e-01,  6.9341e-04],
        [ 9.7499e-02, -3.8919e-01,  3.1459e-04],
        [ 9.6782e-02, -3.8647e-01,  7.3969e-04],
        [ 9.7821e-02, -3.8998e-01,  3.9279e-04],
        [ 9.8025e-02, -3.9250e-01,  2.9540e-04],
        [ 9.7784e-02, -3.9204e-01,  3.3101e-04],
        [ 9.8580e-02, -3.9425e-01,  1.4177e-04],
        [ 9.8590e-02, -3.9125e-01,  1.2058e-04],
        [ 9.8725e-02, -3.9324e-01,  1.5613e-04],
        [ 9.7645e-02, -3.8624e-01,  7.8368e-04],
        [ 9.7800e-02, -3.8847e-01,  5.3057e-04],
        [ 9.7267e-02, -3.8684e-01,  8.2013e-04],
        [ 9.7925e-02, -3.9125e-01,  1.0568e-03],
        [ 9.7880e-02, -3.9027e-01,  7.0232e-04],
        [ 9.8391e-02, -3.9455e-01,  4.3339e-04],
        [ 9.7648e-02, -3.8713e-01,  9.6694e-04],
        [ 9.6685e-02, -3.8678e-01,  1.5985e-03],
        [ 9.2700e-02, -3.7139e-01,  1.7335e-02],
        [ 9.0916e-02, -3.5766e-01,  2.6474e-02],
        [ 8.5713e-02, -3.1819e-01,  4.1707e-02],
        [ 9.0830e-02, -3.7883e-01,  2.1149e-02],
        [ 9.3491e-02, -3.7880e-01,  1.1336e-02],
        [ 9.3806e-02, -3.5775e-01,  9.4592e-03],
        [ 8.3233e-02, -3.6084e-01,  5.2529e-02],
        [ 9.2342e-02, -3.7032e-01,  2.6601e-02],
        [ 7.3043e-02, -2.6398e-01,  2.5266e-01],
        [ 6.8071e-02, -2.8066e-01,  4.2278e-01],
        [ 5.7872e-02, -2.3298e-01,  4.8607e-01],
        [ 5.7986e-02, -2.6261e-01,  4.5922e-01],
        [ 5.0402e-02, -2.0731e-01,  5.5646e-01],
        [ 9.5655e-02, -3.7925e-01,  6.4647e-04],
        [ 9.3733e-02, -3.7744e-01,  1.2485e-03],
        [ 8.8506e-02, -3.3326e-01,  1.0666e-02],
        [ 8.9049e-02, -3.4029e-01,  7.1872e-03],
        [ 9.6274e-02, -3.8568e-01,  7.0795e-04],
        [ 8.9406e-02, -3.8169e-01,  8.6043e-03],
        [ 9.4071e-02, -3.7978e-01,  2.7406e-03],
        [ 9.5791e-02, -3.8547e-01,  5.8264e-04],
        [ 9.5410e-02, -3.7962e-01,  1.6637e-03],
        [ 9.6742e-02, -3.8420e-01,  8.1158e-04],
        [ 9.4066e-02, -3.8670e-01,  2.7366e-03],
        [ 8.9885e-02, -3.7786e-01,  1.0371e-02],
        [ 9.3505e-02, -3.9079e-01,  3.0913e-03],
        [ 9.6220e-02, -3.9037e-01,  1.3367e-03],
        [ 9.4000e-02, -3.4479e-01,  5.1816e-03],
        [ 9.0502e-02, -3.1454e-01,  1.4084e-02],
        [ 9.5036e-02, -3.5414e-01,  1.2024e-02],
        [ 9.5964e-02, -3.6415e-01,  4.2910e-03],
        [ 9.7824e-02, -3.8861e-01,  3.8862e-04],
        [ 9.8613e-02, -3.9231e-01,  1.2195e-04],
        [ 9.5839e-02, -3.8693e-01,  2.3166e-03],
        [ 9.0634e-02, -3.6340e-01,  2.1343e-02],
        [ 9.0850e-02, -3.7964e-01,  1.0639e-02],
        [ 9.4250e-02, -3.8178e-01,  8.2196e-03],
        [ 9.2574e-02, -3.7096e-01,  8.2458e-03],
        [ 9.2919e-02, -3.6198e-01,  1.0473e-02],
        [ 8.7342e-02, -3.6587e-01,  2.1684e-02],
        [ 8.4686e-02, -3.6490e-01,  2.5302e-02],
        [ 9.2758e-02, -3.7939e-01,  1.7905e-02],
        [ 8.0340e-02, -3.1748e-01,  5.1423e-02],
        [ 7.9040e-02, -2.7929e-01,  1.0428e-01],
        [ 9.4192e-02, -3.6678e-01,  6.6460e-03],
        [ 9.4917e-02, -3.9457e-01,  8.4879e-03],
        [ 9.6027e-02, -3.9254e-01,  1.1146e-02],
        [ 9.4186e-02, -3.8845e-01,  1.0717e-02],
        [ 9.1289e-02, -3.8511e-01,  1.2395e-02],
        [ 9.0859e-02, -3.8772e-01,  2.0536e-02],
        [ 8.6322e-02, -3.8027e-01,  2.2691e-02],
        [ 8.7791e-02, -3.8262e-01,  4.2713e-02],
        [ 8.2444e-02, -3.6940e-01,  6.9712e-02],
        [ 8.8923e-02, -3.8188e-01,  2.8547e-02],
        [ 7.7526e-02, -3.2902e-01,  1.4689e-01],
        [ 6.8215e-02, -2.9962e-01,  2.5600e-01]], device='cuda:0',
       grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 10. Compute the negative critic values using the real critic
# 11. Optimize actor
# 12. Update target networks
Run No. 3
Episode Length = 100
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1293,  1.1361,  1.1332,  1.1392,  1.1346,  1.1375,  1.1348,  1.1356,
         1.1364,  1.1325,  1.1301,  1.1351,  1.1299,  1.1328,  1.1286,  1.1225,
         1.1263,  1.1342,  1.1333,  1.1268,  1.1337,  1.1395,  1.1353,  1.1352,
         1.1364,  1.1346,  1.1275,  1.1305, -8.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.1330,  1.1333,  1.1325,  1.1313,  1.1299,
         1.1256,  1.1263, -6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1246,
         1.1253,  1.1293,  1.1358,  1.1399,  1.1395,  1.1324,  1.1319,  1.1341,
         1.1304,  1.0000], device='cuda:0')
target_q_episode tensor([13.3408, 12.9904, 12.6214, 12.2331, 11.8243, 11.3940, 10.9411, 10.4643,
         9.9624,  9.4341,  8.8780,  8.2926,  7.6765,  7.0278,  6.3451,  5.6264,
         4.8699,  4.0736,  3.2354,  2.3530,  1.4242,  0.4466, -0.5826, -1.6659,
        -2.8062, -4.0065, -5.2700, -6.6000, -8.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.8876, -0.1183, -1.1772, -2.2917,
        -3.4650, -4.7000, -6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         8.0253,  7.3950,  6.7316,  6.0333,  5.2982,  4.5244,  3.7099,  2.8525,
         1.9500,  1.0000], device='cuda:0')
target_q tensor([ 9.3149,  9.0823,  8.8340,  8.5757,  8.3001,  8.0126,  7.7081,  7.3888,
         7.0527,  6.6972,  6.3237,  5.9329,  5.5182,  5.0844,  4.6253,  4.1416,
         3.6357,  3.1046,  2.5424,  1.9488,  1.3284,  0.6750, -0.0162, -0.7424,
        -1.5064, -2.3116, -3.1609, -4.0514, -8.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0438,  0.9686,  0.2940, -0.4161, -1.1637,
        -1.9516, -2.7792, -6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0411,
         5.7505,  5.3293,  4.8868,  4.4200,  3.9271,  3.4061,  2.8600,  2.2860,
         1.6798,  1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 0.0477, -0.1680,  0.1492],
        [ 0.0469, -0.2257,  0.1566],
        [ 0.0770, -0.3290,  0.0178],
        [ 0.0820, -0.3376,  0.0153],
        [ 0.0880, -0.3767,  0.0048],
        [ 0.0659, -0.2859,  0.0806],
        [ 0.0599, -0.2386,  0.0862],
        [ 0.0830, -0.3104,  0.0156],
        [ 0.0743, -0.2913,  0.0331],
        [ 0.0820, -0.2974,  0.0243],
        [ 0.0815, -0.3259,  0.0229],
        [ 0.0850, -0.3250,  0.0228],
        [ 0.0704, -0.2512,  0.0856],
        [ 0.0779, -0.3228,  0.0267],
        [ 0.0812, -0.3372,  0.0121],
        [ 0.0802, -0.3190,  0.0242],
        [ 0.0889, -0.3606,  0.0062],
        [ 0.0869, -0.3583,  0.0098],
        [ 0.0886, -0.3581,  0.0152],
        [ 0.0884, -0.3456,  0.0169],
        [ 0.0896, -0.3622,  0.0103],
        [ 0.0929, -0.3814,  0.0038],
        [ 0.0925, -0.3781,  0.0046],
        [ 0.0918, -0.3731,  0.0108],
        [ 0.0872, -0.3517,  0.0211],
        [ 0.0881, -0.3645,  0.0178],
        [ 0.0818, -0.3448,  0.0489],
        [ 0.0848, -0.3418,  0.0437],
        [ 0.0853, -0.3556,  0.0295],
        [ 0.0830, -0.3278,  0.0608],
        [ 0.0828, -0.3427,  0.0545],
        [ 0.0861, -0.3358,  0.0329],
        [ 0.0879, -0.3609,  0.0429],
        [ 0.0882, -0.3581,  0.0338],
        [ 0.0846, -0.3528,  0.0692],
        [ 0.0832, -0.3484,  0.0956],
        [ 0.0827, -0.3385,  0.1373],
        [ 0.0783, -0.3114,  0.1621],
        [ 0.0757, -0.2912,  0.1856],
        [ 0.0722, -0.2961,  0.2420],
        [ 0.0690, -0.2863,  0.2545],
        [ 0.0731, -0.2897,  0.2557],
        [ 0.0563, -0.2468,  0.1151],
        [ 0.0215, -0.0894,  0.3619],
        [ 0.0342, -0.1118,  0.2613],
        [ 0.0520, -0.2350,  0.0783],
        [ 0.0538, -0.1474,  0.1225],
        [ 0.0370, -0.1149,  0.2606],
        [ 0.0421, -0.1428,  0.2225],
        [ 0.0410, -0.1569,  0.2724],
        [ 0.0395, -0.1303,  0.2879],
        [ 0.0452, -0.1812,  0.1683],
        [ 0.0649, -0.2413,  0.0511],
        [ 0.0710, -0.2946,  0.0493],
        [ 0.0748, -0.2743,  0.0604],
        [ 0.0771, -0.2768,  0.0437],
        [ 0.0733, -0.2528,  0.0473],
        [ 0.0794, -0.2671,  0.0339],
        [ 0.0849, -0.3420,  0.0196],
        [ 0.0879, -0.3860,  0.0089],
        [ 0.0915, -0.3894,  0.0085],
        [ 0.0890, -0.3805,  0.0202],
        [ 0.0895, -0.3917,  0.0043],
        [ 0.0921, -0.3937,  0.0046],
        [ 0.0864, -0.3670,  0.0366],
        [ 0.0891, -0.2792,  0.0122],
        [ 0.0829, -0.2664,  0.0220],
        [ 0.0829, -0.3035,  0.0323],
        [ 0.0801, -0.2590,  0.0446],
        [ 0.0815, -0.2667,  0.0277],
        [ 0.0799, -0.2493,  0.0435],
        [ 0.0832, -0.2548,  0.0337],
        [ 0.0821, -0.2031,  0.0442],
        [ 0.0926, -0.2299,  0.0077],
        [ 0.0940, -0.1675,  0.0127],
        [ 0.0922, -0.1160,  0.0141],
        [ 0.0905, -0.1083,  0.0302],
        [ 0.0898, -0.1732,  0.0368],
        [ 0.0916, -0.2303,  0.0194],
        [ 0.0881, -0.2106,  0.0454],
        [ 0.0856, -0.2681,  0.0314],
        [ 0.0861, -0.2906,  0.0353],
        [ 0.0927, -0.3058,  0.0112],
        [ 0.0865, -0.3404,  0.0344],
        [ 0.0928, -0.3720,  0.0185],
        [ 0.0934, -0.3839,  0.0216],
        [ 0.0929, -0.3840,  0.0340],
        [ 0.0921, -0.3851,  0.0253],
        [ 0.0803, -0.2736,  0.0757],
        [ 0.0889, -0.3114,  0.0346],
        [ 0.0853, -0.3212,  0.0416],
        [ 0.0789, -0.3293,  0.1203],
        [ 0.0804, -0.3478,  0.0672],
        [ 0.0807, -0.3456,  0.0585],
        [ 0.0763, -0.3390,  0.1218],
        [ 0.0713, -0.3406,  0.1429],
        [ 0.0751, -0.3103,  0.1023],
        [ 0.0662, -0.2944,  0.2671],
        [ 0.0709, -0.2972,  0.2098],
        [ 0.0682, -0.3192,  0.1272]], device='cuda:0', grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 11. Optimize actor
# 12. Update target networks
Run No. 4
Episode Length = 81
# 1. Compute target actions from target actor P'(s(t+1))
>>> # 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([1.1417, 1.1437, 1.1486, 1.1471, 1.1304, 1.1351, 1.1338, 1.1362, 1.1390,
        1.1424, 1.1391, 1.1367, 1.1357, 1.1415, 1.1391, 1.1455, 1.1452, 1.1424,
        1.1413, 1.1421, 1.1406, 1.1386, 1.1386, 1.1391, 1.1427, 1.1453, 1.1468,
        1.1478, 1.1386, 1.1477, 1.1396, 1.1503, 1.1464, 1.1436, 1.1478, 1.1499,
        1.1411, 1.1446, 1.1412, 1.1399, 1.1416, 1.1447, 1.0000],
       device='cuda:0')
target_q_episode tensor([17.7963, 17.6804, 17.5583, 17.4298, 17.2945, 17.1521, 17.0022, 16.8444,
        16.6783, 16.5035, 16.3195, 16.1258, 15.9219, 15.7072, 15.4813, 15.2435,
        14.9931, 14.7296, 14.4522, 14.1602, 13.8529, 13.5293, 13.1888, 12.8303,
        12.4529, 12.0557, 11.6376, 11.1975, 10.7342, 10.2465,  9.7332,  9.1928,
         8.6240,  8.0253,  7.3950,  6.7316,  6.0333,  5.2982,  4.5244,  3.7099,
         2.8525,  1.9500,  1.0000], device='cuda:0')
target_q tensor([10.2819, 10.2192, 10.1544, 10.0832, 10.0014,  9.9254,  9.8425,  9.7571,
         9.6672,  9.5728,  9.4703,  9.3629,  9.2505,  9.1353,  9.0103,  8.8826,
         8.7451,  8.5992,  8.4465,  8.2866,  8.1172,  7.9388,  7.7519,  7.5553,
         7.3499,  7.1331,  6.9043,  6.6632,  6.4048,  6.1412,  5.8558,  5.5641,
         5.2502,  4.9203,  4.5763,  4.2132,  3.8260,  3.4241,  2.9979,  2.5503,
         2.0806,  1.5867,  1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 9.1728e-02, -3.6851e-01,  4.3596e-03],
        [ 9.3340e-02, -3.7136e-01,  3.2195e-03],
        [ 9.3753e-02, -3.6985e-01,  3.8512e-03],
        [ 9.4798e-02, -3.8944e-01,  1.6578e-03],
        [ 9.5508e-02, -3.8767e-01,  1.3881e-03],
        [ 9.6172e-02, -3.8687e-01,  9.3868e-04],
        [ 9.7009e-02, -3.8887e-01,  6.6555e-04],
        [ 9.5702e-02, -3.8729e-01,  1.0015e-03],
        [ 9.7354e-02, -3.9167e-01,  6.6945e-04],
        [ 9.6442e-02, -3.8922e-01,  1.0957e-03],
        [ 9.6324e-02, -3.8627e-01,  1.3716e-03],
        [ 9.8244e-02, -3.9355e-01,  3.3775e-04],
        [ 9.7282e-02, -3.9182e-01,  7.3624e-04],
        [ 9.8295e-02, -3.9274e-01,  1.8921e-04],
        [ 9.8322e-02, -3.9442e-01,  2.1395e-04],
        [ 9.8120e-02, -3.9456e-01,  3.3844e-04],
        [ 9.8958e-02, -3.9543e-01,  2.7737e-04],
        [ 9.7627e-02, -3.8691e-01,  1.1099e-03],
        [ 9.8573e-02, -3.9621e-01,  2.4524e-04],
        [ 9.7352e-02, -3.9185e-01,  1.2232e-03],
        [ 9.7949e-02, -3.9324e-01,  9.4122e-04],
        [ 9.8258e-02, -3.9638e-01,  6.5050e-04],
        [ 9.7560e-02, -3.9091e-01,  1.2531e-03],
        [ 9.7341e-02, -3.9151e-01,  1.2420e-03],
        [ 9.6231e-02, -3.8857e-01,  3.7377e-03],
        [ 9.3410e-02, -3.6868e-01,  1.5536e-02],
        [ 8.9183e-02, -3.7232e-01,  3.1506e-02],
        [ 8.7555e-02, -3.5396e-01,  4.4541e-02],
        [ 8.8704e-02, -3.6191e-01,  4.4468e-02],
        [ 8.6623e-02, -3.5360e-01,  4.1600e-02],
        [ 9.0718e-02, -3.5219e-01,  2.6248e-02],
        [ 8.0366e-02, -3.5909e-01,  6.8668e-02],
        [ 9.3425e-02, -3.8011e-01,  1.9638e-02],
        [ 6.8547e-02, -2.9241e-01,  2.6840e-01],
        [ 6.3751e-02, -2.5531e-01,  4.1940e-01],
        [ 5.7698e-02, -2.5507e-01,  4.5154e-01],
        [ 5.5268e-02, -2.5309e-01,  4.8403e-01],
        [ 4.8253e-02, -2.2059e-01,  5.5899e-01],
        [ 9.0228e-02, -3.5999e-01,  4.9396e-03],
        [ 8.9951e-02, -3.5420e-01,  6.4757e-03],
        [ 9.3309e-02, -3.7617e-01,  4.7967e-03],
        [ 8.7995e-02, -3.3314e-01,  1.7593e-02],
        [ 7.6540e-02, -3.0209e-01,  7.3408e-02],
        [ 7.4061e-02, -3.3860e-01,  5.3500e-02],
        [ 8.2076e-02, -3.3828e-01,  3.1619e-02],
        [ 9.5684e-02, -3.8800e-01,  7.6681e-04],
        [ 8.8462e-02, -3.7619e-01,  1.1749e-02],
        [ 9.4561e-02, -3.8788e-01,  3.3851e-03],
        [ 9.7413e-02, -3.9337e-01,  5.9739e-04],
        [ 9.5588e-02, -3.8179e-01,  2.3719e-03],
        [ 9.7590e-02, -3.9253e-01,  3.7888e-04],
        [ 9.5533e-02, -3.8854e-01,  1.9466e-03],
        [ 9.6859e-02, -3.9164e-01,  1.1825e-03],
        [ 9.7272e-02, -3.9405e-01,  6.8778e-04],
        [ 9.7357e-02, -3.9040e-01,  9.0536e-04],
        [ 9.5363e-02, -3.7001e-01,  4.9024e-03],
        [ 9.4364e-02, -3.5834e-01,  1.4873e-02],
        [ 9.5072e-02, -3.5459e-01,  5.0090e-03],
        [ 9.1203e-02, -3.5124e-01,  1.0058e-02],
        [ 9.7074e-02, -3.8906e-01,  1.0071e-03],
        [ 9.7727e-02, -3.9023e-01,  5.7268e-04],
        [ 9.3964e-02, -3.8401e-01,  5.0842e-03],
        [ 9.0710e-02, -3.6151e-01,  1.7675e-02],
        [ 9.5934e-02, -3.8392e-01,  5.2438e-03],
        [ 9.7609e-02, -3.9003e-01,  9.5743e-04],
        [ 9.2309e-02, -3.6013e-01,  1.8978e-02],
        [ 8.1985e-02, -3.4423e-01,  4.5546e-02],
        [ 8.7442e-02, -3.6994e-01,  2.3369e-02],
        [ 8.6245e-02, -3.6086e-01,  3.8668e-02],
        [ 8.8987e-02, -3.4890e-01,  4.0502e-02],
        [ 9.1042e-02, -3.8960e-01,  1.4531e-02],
        [ 9.0332e-02, -3.9423e-01,  1.1379e-02],
        [ 9.2621e-02, -3.9164e-01,  1.6379e-02],
        [ 8.6706e-02, -3.6489e-01,  4.7571e-02],
        [ 8.6912e-02, -3.7608e-01,  2.5939e-02],
        [ 9.0549e-02, -3.7328e-01,  3.1529e-02],
        [ 8.0471e-02, -3.5682e-01,  1.0293e-01],
        [ 8.4297e-02, -3.8710e-01,  4.0647e-02],
        [ 8.5206e-02, -3.7943e-01,  4.9185e-02],
        [ 7.0542e-02, -3.2175e-01,  1.7402e-01],
        [ 5.7871e-02, -2.4395e-01,  4.5842e-01]], device='cuda:0',
       grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 10. Compute the negative critic values using the real critic
# 11. Optimize actor
# 12. Update target networks
>>> Run No. 5
Episode Length = 92
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1445,  1.1432,  1.1535,  1.1523,  1.1451,  1.1405,  1.1407,  1.1482,
         1.1551,  1.1544,  1.1474,  1.1332, -3.0000,  0.0000,  0.1422,  1.1436,
         1.1437,  1.1416,  1.1424,  1.1378,  1.1434,  1.1529,  1.1578,  1.1461,
         1.1477,  1.1472,  1.1544,  1.1509,  1.1475,  1.1529,  1.1447,  1.1481,
         1.1501,  1.1529,  1.1502,  1.1409, -4.0000,  0.0000,  0.0000,  0.1428,
         1.1427,  1.1472,  1.1271, -3.0000,  0.0000,  0.1366,  1.1458,  1.1438,
         1.1450,  1.0000], device='cuda:0')
target_q_episode tensor([ 7.5717,  6.9176,  6.2291,  5.5043,  4.7413,  3.9382,  3.0929,  2.2030,
         1.2664,  0.2804, -0.7575, -1.8500, -3.0000,  0.0000,  0.0000, 11.8265,
        11.3963, 10.9435, 10.4669,  9.9651,  9.4370,  8.8810,  8.2958,  7.6798,
         7.0314,  6.3488,  5.6303,  4.8740,  4.0779,  3.2399,  2.3578,  1.4293,
         0.4519, -0.5770, -1.6600, -2.8000, -4.0000,  0.0000,  0.0000,  0.0000,
         0.2804, -0.7575, -1.8500, -3.0000,  0.0000,  0.0000,  3.7099,  2.8525,
         1.9500,  1.0000], device='cuda:0')
target_q tensor([ 4.0324,  3.7378,  3.4341,  3.1078,  2.7610,  2.3976,  2.0179,  1.6221,
         1.2051,  0.7617,  0.2915, -0.2072, -3.0000,  0.0000,  0.0783,  5.9437,
         5.7505,  5.5459,  5.3321,  5.1042,  4.8699,  4.6254,  4.3651,  4.0819,
         3.7914,  3.4844,  3.1655,  2.8238,  2.4642,  2.0907,  1.6898,  1.2744,
         0.8364,  0.3756, -0.1125, -0.6298, -4.0000,  0.0000,  0.0000,  0.0786,
         0.7552,  0.2914, -0.2106, -3.0000,  0.0000,  0.0752,  2.2979,  1.9116,
         1.5067,  1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 0.0735, -0.2839,  0.0234],
        [ 0.0728, -0.2814,  0.0260],
        [ 0.0811, -0.3008,  0.0167],
        [ 0.0752, -0.2896,  0.0355],
        [ 0.0850, -0.3227,  0.0158],
        [ 0.0842, -0.3372,  0.0130],
        [ 0.0804, -0.3201,  0.0254],
        [ 0.0774, -0.2531,  0.0316],
        [ 0.0603, -0.2016,  0.1174],
        [ 0.0518, -0.1683,  0.1839],
        [ 0.0552, -0.1839,  0.1580],
        [ 0.0591, -0.2178,  0.1106],
        [ 0.0606, -0.2107,  0.0883],
        [ 0.0676, -0.2236,  0.0410],
        [ 0.0707, -0.2429,  0.0424],
        [ 0.0818, -0.3068,  0.0159],
        [ 0.0830, -0.2733,  0.0144],
        [ 0.0859, -0.3136,  0.0137],
        [ 0.0881, -0.3401,  0.0095],
        [ 0.0877, -0.3512,  0.0094],
        [ 0.0862, -0.3232,  0.0148],
        [ 0.0906, -0.3537,  0.0066],
        [ 0.0861, -0.3567,  0.0199],
        [ 0.0891, -0.3463,  0.0154],
        [ 0.0845, -0.3464,  0.0198],
        [ 0.0897, -0.3704,  0.0149],
        [ 0.0877, -0.3773,  0.0090],
        [ 0.0874, -0.3408,  0.0106],
        [ 0.0810, -0.3489,  0.0400],
        [ 0.0894, -0.3822,  0.0140],
        [ 0.0879, -0.3372,  0.0327],
        [ 0.0872, -0.3645,  0.0219],
        [ 0.0878, -0.3656,  0.0326],
        [ 0.0972, -0.3967,  0.0017],
        [ 0.0946, -0.3943,  0.0072],
        [ 0.0970, -0.3976,  0.0028],
        [ 0.0963, -0.3956,  0.0035],
        [ 0.0971, -0.3956,  0.0039],
        [ 0.0922, -0.3813,  0.0143],
        [ 0.0878, -0.3691,  0.0416],
        [ 0.0704, -0.2816,  0.1478],
        [ 0.0562, -0.1973,  0.4000],
        [ 0.0288, -0.0559,  0.2868],
        [ 0.0310, -0.0737,  0.2328],
        [ 0.0684, -0.2662,  0.0408],
        [ 0.0521, -0.1662,  0.1040],
        [ 0.0376, -0.1335,  0.2622],
        [ 0.0427, -0.1452,  0.2401],
        [ 0.0762, -0.2761,  0.0197],
        [ 0.0752, -0.3060,  0.0333],
        [ 0.0726, -0.2736,  0.0375],
        [ 0.0446, -0.1416,  0.2157],
        [ 0.0715, -0.3318,  0.0377],
        [ 0.0777, -0.2986,  0.0238],
        [ 0.0796, -0.3470,  0.0231],
        [ 0.0792, -0.2780,  0.0166],
        [ 0.0740, -0.3301,  0.0448],
        [ 0.0823, -0.2829,  0.0252],
        [ 0.0682, -0.1540,  0.0820],
        [ 0.0704, -0.1836,  0.0519],
        [ 0.0754, -0.1863,  0.0538],
        [ 0.0812, -0.1824,  0.0326],
        [ 0.0787, -0.1247,  0.0324],
        [ 0.0766, -0.2734,  0.0449],
        [ 0.0796, -0.1517,  0.0391],
        [ 0.0864, -0.3570,  0.0086],
        [ 0.0801, -0.2579,  0.0333],
        [ 0.0850, -0.3271,  0.0233],
        [ 0.0825, -0.3434,  0.0415],
        [ 0.0786, -0.3146,  0.0634],
        [ 0.0852, -0.3483,  0.0297],
        [ 0.0886, -0.3644,  0.0195],
        [ 0.0784, -0.3311,  0.0327],
        [ 0.0844, -0.3522,  0.0330],
        [ 0.0850, -0.3365,  0.0305],
        [ 0.0899, -0.3710,  0.0188],
        [ 0.0896, -0.3846,  0.0220],
        [ 0.0904, -0.3865,  0.0120],
        [ 0.0907, -0.3677,  0.0322],
        [ 0.0745, -0.2957,  0.0934],
        [ 0.0880, -0.3413,  0.0410],
        [ 0.0960, -0.3970,  0.0040],
        [ 0.0954, -0.3964,  0.0048],
        [ 0.0949, -0.3927,  0.0088],
        [ 0.0831, -0.3601,  0.0775],
        [ 0.0857, -0.3729,  0.0597],
        [ 0.0822, -0.3743,  0.0513],
        [ 0.0855, -0.3502,  0.0725],
        [ 0.0826, -0.3137,  0.1008],
        [ 0.0726, -0.3069,  0.1824],
        [ 0.0745, -0.2968,  0.1154],
        [ 0.0686, -0.2896,  0.2447]], device='cuda:0', grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 11. Optimize actor
# 12. Update target networks
Run No. 6
Episode Length = 82
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
>>> # 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1651,  1.1645,  1.1458,  1.1574,  1.1554,  1.1571,  1.1623,  1.1669,
         1.1722,  1.1676,  1.1647,  1.1615,  1.1600,  1.1615,  1.1607,  1.1649,
         1.1630,  1.1635,  1.1612,  1.1615,  1.1642,  1.1571,  1.1563,  1.1579,
         1.1610,  1.1608,  1.1655,  1.1678,  1.1630,  1.1600,  1.1602,  1.1493,
         1.1649,  1.1609,  1.1646,  1.1686,  1.1708,  1.1595,  1.1485, -5.0000,
         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0')
target_q_episode tensor([16.6181, 16.4401, 16.2527, 16.0555, 15.8479, 15.6294, 15.3994, 15.1572,
        14.9023, 14.6340, 14.3516, 14.0543, 13.7414, 13.4120, 13.0653, 12.7003,
        12.3161, 11.9117, 11.4860, 11.0379, 10.5662, 10.0696,  9.5470,  8.9968,
         8.4177,  7.8081,  7.1664,  6.4910,  5.7800,  5.0316,  4.2438,  3.4145,
         2.5416,  1.6227,  0.6555, -0.3627, -1.4344, -2.5625, -3.7500, -5.0000,
         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0')
target_q tensor([ 6.8499,  6.7841,  6.7034,  6.6381,  6.5605,  6.4812,  6.3998,  6.3137,
         6.2232,  6.1216,  6.0159,  5.9045,  5.7884,  5.6682,  5.5402,  5.4085,
         5.2660,  5.1176,  4.9595,  4.7948,  4.6230,  4.4359,  4.2431,  4.0417,
         3.8306,  3.6062,  3.3731,  3.1261,  2.8615,  2.5843,  2.2946,  1.9826,
         1.6713,  1.3308,  0.9773,  0.6053,  0.2124, -0.2097, -0.6535, -5.0000,
         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
# 7.2 Optimize critic
# 8. Compute actor actions
actor actions tensor([[ 9.0452e-02, -3.4408e-01,  5.6233e-03],
        [ 9.3489e-02, -3.4447e-01,  2.6317e-03],
        [ 8.9294e-02, -3.3305e-01,  8.3100e-03],
        [ 9.6317e-02, -3.9002e-01,  5.1633e-04],
        [ 9.5470e-02, -3.8455e-01,  1.0321e-03],
        [ 9.6795e-02, -3.8562e-01,  4.4474e-04],
        [ 9.5412e-02, -3.7089e-01,  1.5851e-03],
        [ 9.5900e-02, -3.8641e-01,  6.5553e-04],
        [ 9.7403e-02, -3.8419e-01,  4.3371e-04],
        [ 9.5254e-02, -3.7495e-01,  1.1323e-03],
        [ 9.7671e-02, -3.8810e-01,  4.5404e-04],
        [ 9.7927e-02, -3.9336e-01,  1.9783e-04],
        [ 9.7078e-02, -3.8788e-01,  5.8287e-04],
        [ 9.8194e-02, -3.9260e-01,  3.2866e-04],
        [ 9.8515e-02, -3.9189e-01,  2.0820e-04],
        [ 9.8007e-02, -3.8827e-01,  3.2175e-04],
        [ 9.8166e-02, -3.8661e-01,  3.7014e-04],
        [ 9.8123e-02, -3.8856e-01,  2.7391e-04],
        [ 9.8030e-02, -3.8742e-01,  3.7053e-04],
        [ 9.7385e-02, -3.8240e-01,  9.2793e-04],
        [ 9.7758e-02, -3.8503e-01,  8.8006e-04],
        [ 9.7521e-02, -3.9168e-01,  8.5074e-04],
        [ 9.7936e-02, -3.9281e-01,  5.1799e-04],
        [ 9.6501e-02, -3.8486e-01,  1.4510e-03],
        [ 9.5960e-02, -3.7939e-01,  2.4262e-03],
        [ 9.1543e-02, -3.6363e-01,  1.2319e-02],
        [ 9.2613e-02, -3.5429e-01,  1.2930e-02],
        [ 8.9452e-02, -2.9971e-01,  2.1619e-02],
        [ 9.1038e-02, -3.6094e-01,  2.2217e-02],
        [ 9.3721e-02, -3.7998e-01,  7.7127e-03],
        [ 9.3130e-02, -3.5272e-01,  9.2393e-03],
        [ 7.8890e-02, -3.4100e-01,  4.6185e-02],
        [ 9.1984e-02, -3.5933e-01,  1.7767e-02],
        [ 6.3352e-02, -2.1781e-01,  2.8109e-01],
        [ 6.5046e-02, -2.5746e-01,  2.7881e-01],
        [ 5.7043e-02, -2.2149e-01,  3.6471e-01],
        [ 5.8334e-02, -2.2450e-01,  3.4145e-01],
        [ 5.4669e-02, -2.3515e-01,  4.3660e-01],
        [ 9.0241e-02, -3.6779e-01,  4.4827e-03],
        [ 9.5251e-02, -3.7346e-01,  1.8477e-03],
        [ 8.3123e-02, -3.4705e-01,  2.0295e-02],
        [ 7.7438e-02, -3.1707e-01,  3.4034e-02],
        [ 7.4666e-02, -3.1513e-01,  2.8605e-02],
        [ 7.7666e-02, -2.5248e-01,  5.1727e-02],
        [ 9.5935e-02, -3.8907e-01,  7.8988e-04],
        [ 9.4423e-02, -3.7508e-01,  1.2478e-03],
        [ 9.3215e-02, -3.6306e-01,  3.9727e-03],
        [ 7.9242e-02, -2.3313e-01,  5.1026e-02],
        [ 8.0673e-02, -2.8346e-01,  4.2415e-02],
        [ 7.6722e-02, -2.5742e-01,  3.9200e-02],
        [ 8.0484e-02, -3.1481e-01,  3.2700e-02],
        [ 9.4565e-02, -3.6796e-01,  1.5694e-03],
        [ 9.7511e-02, -3.9122e-01,  2.8506e-04],
        [ 9.8102e-02, -3.9563e-01,  1.6099e-04],
        [ 9.6161e-02, -3.8123e-01,  1.5332e-03],
        [ 9.7354e-02, -3.7810e-01,  6.8474e-04],
        [ 9.6719e-02, -3.8238e-01,  9.3925e-04],
        [ 9.6822e-02, -3.8117e-01,  7.6231e-04],
        [ 9.8213e-02, -3.8989e-01,  2.8849e-04],
        [ 9.7467e-02, -3.8774e-01,  4.8307e-04],
        [ 9.6929e-02, -3.8800e-01,  8.1989e-04],
        [ 9.3902e-02, -3.7338e-01,  5.9229e-03],
        [ 9.3759e-02, -3.8595e-01,  3.8244e-03],
        [ 9.1547e-02, -3.5615e-01,  1.2962e-02],
        [ 9.5068e-02, -3.7978e-01,  3.9493e-03],
        [ 9.1455e-02, -3.4082e-01,  9.5353e-03],
        [ 8.7000e-02, -3.3804e-01,  3.0295e-02],
        [ 8.4912e-02, -3.5215e-01,  2.3244e-02],
        [ 9.0778e-02, -3.6782e-01,  1.2660e-02],
        [ 9.2388e-02, -3.7775e-01,  1.0580e-02],
        [ 9.2730e-02, -3.7683e-01,  1.8790e-02],
        [ 8.6347e-02, -3.7206e-01,  5.4624e-02],
        [ 8.9698e-02, -3.8323e-01,  1.5038e-02],
        [ 9.7155e-02, -3.9768e-01,  1.7609e-03],
        [ 8.9903e-02, -3.7829e-01,  1.2661e-02],
        [ 8.7962e-02, -3.6674e-01,  2.4981e-02],
        [ 8.4594e-02, -3.6544e-01,  5.3366e-02],
        [ 7.9409e-02, -3.6387e-01,  6.0108e-02],
        [ 8.0390e-02, -3.8391e-01,  5.1338e-02],
        [ 8.6808e-02, -3.8265e-01,  3.6508e-02],
        [ 9.0753e-02, -3.9055e-01,  1.2648e-02],
        [ 8.0063e-02, -3.0616e-01,  7.3940e-02]], device='cuda:0',
       grad_fn=<AddBackward0>)
# 9. Compute actor loss
# 10. Compute the negative critic values using the real critic
# 11. Optimize actor
>>> # 12. Update target networks
Run No. 7
Episode Length = 99
# 1. Compute target actions from target actor P'(s(t+1))
# 2. Compute Q-value of next state using the  target critics Q'(s(t+1), P'(s(t+1)))
# 3. Use smaller Q-value as the Q-value target
# 4. Compute current Q-value with the reward
target_q_critic tensor([ 1.1706,  1.1622,  1.1484,  1.1508,  1.1479,  1.1524,  1.1531,  1.1527,
        -7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1667,  1.1851,
         1.1850,  1.1775,  1.1589,  1.1574,  1.1638,  1.1822,  1.1888,  1.1632,
         1.1517,  1.1603,  1.1650,  1.1718,  1.1809,  1.1645,  1.1655,  1.1663,
         1.1625,  1.1663,  1.1684,  1.1691,  1.1619,  1.1542, -4.0000,  0.0000,
         0.0000,  0.1536,  1.1427,  1.1528,  1.1521, -6.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.1567,  1.1533,  1.1587, -2.0000,  0.1663,  1.1618,
         1.0000], device='cuda:0')
target_q_episode tensor([ 2.0876,  1.1449,  0.1525, -0.8921, -1.9917, -3.1491, -4.3675, -5.6500,
        -7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.6234,
        12.2352, 11.8265, 11.3963, 10.9435, 10.4669,  9.9651,  9.4370,  8.8810,
         8.2958,  7.6798,  7.0314,  6.3488,  5.6303,  4.8740,  4.0779,  3.2399,
         2.3578,  1.4293,  0.4519, -0.5770, -1.6600, -2.8000, -4.0000,  0.0000,
         0.0000,  0.0000, -2.2917, -3.4650, -4.7000, -6.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.1450, -0.9000, -2.0000,  0.0000,  1.9500,
         1.0000], device='cuda:0')
target_q tensor([ 1.4468,  1.1570,  0.8485,  0.5355,  0.2023, -0.1432, -0.5097, -0.8962,
        -7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1165,  4.6303,
         4.5133,  4.3849,  4.2424,  4.1049,  3.9658,  3.8275,  3.6731,  3.4877,
         3.3035,  3.1240,  2.9319,  2.7311,  2.5210,  2.2818,  2.0427,  1.7908,
         1.5225,  1.2455,  0.9526,  0.6432,  0.3119, -0.0368, -4.0000,  0.0000,
         0.0000,  0.1073,  0.1083, -0.2381, -0.6105, -6.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.1095,  0.8496,  0.5386, -2.0000,  0.1162,  1.3992,
         1.0000], device='cuda:0')
# 5.1 Compute Q-value from critics Q(s_t, a_t)
# 6.1 Compute MSE loss for the critics
# 7.1 Optimize critic
# 5.2 Compute Q-value from critics Q(s_t, a_t)
# 6.2 Compute MSE loss for the critics
>>> >>>
>>> >>>
KeyboardInterrupt
[1m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1mKeyboardInterrupt